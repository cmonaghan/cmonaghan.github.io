---
layout: post
title:  "100 Days of Gen AI: Day 31"
---

I started work on a prototype for LLM prompt guardrails. It filters for PII and masks it before sending the query on to the model for inference. It can also block requests entirely. I'd like to apply the filter then displayt to the user what was masked or blocked.

![Prompt guardrail work in progress](/assets/2025-01-08-prompt-guardrail.png)
